{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOTvw4o64jpK"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic==0.14.1\n",
        "!pip install --upgrade joblib==1.1.0\n",
        "!pip install --upgrade tbb\n",
        "!pip install stanza==1.4.0\n",
        "!pip install jsonlines\n",
        "!pip install jiwer==2.2.1\n",
        "!pip install spacy[transformers]\n",
        "!python -m spacy download en_core_web_sm\n",
        "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX8kNoR88xVr",
        "outputId": "1e1e981e-fc69-477d-ce0a-51703d76d3df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import random\n",
        "import transformers\n",
        "transformers.set_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import ast\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "import jiwer\n",
        "\n",
        "import gensim\n",
        "import pprint\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util, models\n",
        "cross_encoder = CrossEncoder('cross-encoder/stsb-distilroberta-base',device=device)\n",
        "entail_encoder = CrossEncoder('cross-encoder/nli-distilroberta-base',device=device)\n",
        "\n",
        "# sentence-transformers/all-mpnet-base-v2\n",
        "word_embedding_model = SentenceTransformer('flax-sentence-embeddings/stackoverflow_mpnet-base',device=device)\n",
        "\n",
        "\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.patches as mpatches\n",
        "plt.rcParams[\"figure.figsize\"] = (11.7,8.27)\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "main_arguments = ['ARG' + str(idx) for idx in range(6)]\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from hdbscan import HDBSCAN\n",
        "from umap import UMAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7nVZ37Dm_KW",
        "outputId": "9f6cfd9c-50c5-4c59-a913-dbb86f185b34"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FToHaX-Wo6B8"
      },
      "outputs": [],
      "source": [
        "def process_(text):\n",
        "  text = text.replace(\"\\r\\n\",\" \").replace(\"\\n\",\" \")\n",
        "  text = re.sub('podling', 'project', text,flags=re.IGNORECASE)\n",
        "  regex = r'\\b(\\w+)(?:\\W+\\1\\b)+'\n",
        "  text = re.sub(regex, r'\\1', text, flags=re.IGNORECASE)\n",
        "  #remove URL\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "  return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "on4yQNnAOpP5",
        "outputId": "4f434279-17cc-4d60-fb61-ff51e7758804"
      },
      "outputs": [],
      "source": [
        "# only topic rules\n",
        "\n",
        "rules_data = pd.read_csv('asf_rules.csv')\n",
        "rules = rules_data['rules'].tolist();print(len(rules))\n",
        "topic_model = BERTopic.load('topic_model',embedding_model=word_embedding_model)\n",
        "print(\"cluster density:\",topic_model.hdbscan_model.relative_validity_)\n",
        "\n",
        "\n",
        "freq = topic_model.get_topic_info();print('No of topics',freq.shape[0])\n",
        "asf_tops,_ = topic_model.transform(rules)\n",
        "top_map = pd.DataFrame(columns=['rule','label'])\n",
        "top_map['rules'],top_map['label'] = rules,asf_tops\n",
        "top_map['topic'] = top_map['label'].apply(lambda x : freq[freq['Topic'] == x].iloc[0]['Name'])\n",
        "top_map.sort_values(by=['label'],inplace=True)\n",
        "top_map.to_csv('cluster_rules_outlier.csv',index=False)\n",
        "top_map = top_map[top_map['label'] > -1];top_map.to_csv('cluster_rules.csv')\n",
        "print('ASF Topics: ', len(top_map['label'].unique()))\n",
        "freq.to_csv('freq.csv',index=False)\n",
        "print(topic_model.hdbscan_model.min_cluster_size,topic_model.hdbscan_model.min_samples,topic_model.umap_model.n_components,topic_model.umap_model.n_neighbors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWp5NtgBHT40"
      },
      "outputs": [],
      "source": [
        "#generate topic predictions using tuned model\n",
        "topic_model = BERTopic.load('topic_model',embedding_model=word_embedding_model)\n",
        "print(\"cluster density:\",topic_model.hdbscan_model.relative_validity_)\n",
        "all_data = pd.read_csv('srl_clauses_processed_en.csv');all_data.shape[0]\n",
        "\n",
        "all_data['parsed_clauses'] = all_data['parsed_clauses'].apply(lambda x : process_(x))\n",
        "all_data['parsed_clauses'] = all_data['parsed_clauses'].apply(lambda x : x if len(x.strip()) > 2 else np.nan); all_data.dropna(subset=['parsed_clauses'],inplace=True)\n",
        "\n",
        "final_data = pd.DataFrame(columns = list(all_data.columns) + ['group_id'])\n",
        "print('All data: ', all_data.shape[0])\n",
        "for df in np.array_split(all_data,20):\n",
        "  print(df.shape[0])\n",
        "  df[\"group_id\"] = topic_model.transform(df['parsed_clauses'].tolist())[0]\n",
        "  final_data = final_data.append(df);print(final_data.shape[0])\n",
        "\n",
        "assert all_data.shape[0] == final_data.shape[0]\n",
        "\n",
        "#keep only parsed activities related to governance topics\n",
        "\n",
        "final_data = final_data[(final_data['group_id'].isin(asf_tops)) & (final_data['group_id'] > -1)]\n",
        "freq = topic_model.get_topic_info()\n",
        "final_data[\"topic\"] = final_data['group_id'].apply(lambda x : freq[freq['Topic'] == x].iloc[0]['Name'])\n",
        "final_data.to_csv('governed_activities.csv'); print(final_data.shape[0])\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
